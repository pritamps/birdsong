HTML header: <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

# Capstone Project
## Machine Learning Engineer Nanodegree
Pritam Sukumar
July 29th 2016

## I. Definition
A friend and I are building an app that will use machine learning to recognise bird sounds from iPhone recordings, and I wanted to use this project as an opportunity to explore work that has already been done in this area. While researching potential topics related to bird sound recognition, I found a Kaggle competition ([MLSP 2013 Bird Classification Challenge](https://www.kaggle.com/c/mlsp-2013-birds) ) that deals with the same exact problem along with a potential approach that I could try out. I decided to go along with the steps and implement the machine learning portions of the competition myself to learn more about this field.

### Project Overview
The recognition of bird sounds holds importance for a variety of reasons. Identifying the bird species in a given region allows scientists to prepare surveys of current populations, along with aiding studies of specific species, investigating the effect of environmental factors such as climate change, and also providing data for making predictions about the future in that area. Also, the hobby of bird watching has grown immensely, and for bird watchers, spotting a new species (or even a known one) is a very pleasing experience.

Identifying birds present in an area for scientific research is an expensive task, both in terms of time and money, due to the necessity of in-the-field researchers. For hobbyists, it has always been a challenge -- and, to be honest, part of the charm -- to recognise new species in the wild. In recent years,  there has been a strong interest in the possibility of using machine learning for this task. 

One approach that was followed (and is explored in this project), is using data from microphones placed in the forest. This has the advantage of not needing human presence to record the sounds. The microphones are retrieved every few weeks and the data is used to identify the bird species present in the recording. In this project, I am going to try to (or rather verify) that it is indeed practical, and that the accuracy that we can obtain using relatively new data-processing and machine learning techniques is very promising for the future of bird-sound recognition and my app.

### Problem Statement
The scope of this project is defined by the scope of the [Kaggle competition](https://www.kaggle.com/c/mlsp-2013-birds). The task is to use machine learning to automatically detect the species of bird in a recording made in the wild. The recordings are usually of short duration. However, they may contain no birds (as is the case for most of the day), or multiple birds in the same recording (as is often the case for recordings taken at dawn or dusk). 

The strategy applied for this task is to understand how to extract information from the sound recordings. This usually entails the following steps:

1. Detecting whether bird sounds exist in the wave file
2. Detecting whether there are one more more birds
3. Separating the individual bird sounds 
4. Processing the bird sounds in a way that is friendly to machine learning classification

I intend to retrace the steps of a paper that accompanies the Kaggle competition. The first step is generating a spectrogram, i.e. converting the audio file to the frequency domain because birds vary much more by pitch than by amplitude (which is what a usual wave file will provide). Once that is done, we reduce the noise in the data, and then separate out the bird sounds from the rest of the audio (the segmentation problem), and also from each other (the cocktail party problem). Then we can run a classifier over the segmented frequency graphs to detect the birds in a given recording.

Note that the problem complexity increases because this a Multi-Instance Multi-Label (MIML) problem, i.e. Every audio file can have multiple bird sounds (instances) in it, and each instance of a bird sound can correspond to one of many different birds (labels).

The final result should be something along the lines of: "In recording *m*, there are three birds present, which are *x,y,* and *z*.

### Metrics
Since I am using a dataset from an expired Kaggle competition, the metrics are defined by the rules of the competition. In this case, the metric is the overall accuracy of the classification as measured by the area under the ROC curve. As defined by the competition, the ROC curve is generated by taking as input the labels generated by the algorithm, along with the predictions.

The ROC (Receiver Operating Curve) is a curve that plots the true positive rate versus the false positive rate for a binary classification algorithm. The AUC (Area Under the Curve) represents the accuracy of the algorithm.

In the case of the bird-classification problem, since the problem is multiclass, we need to first convert it to a binary classification problem. We do this by simply treating the presence of each bird in each recording as a separate instance for the algorithm. This way, every sound recording corresponds to nineteen classifications.

Reframing the problem as a binary problem (Binary Relevance) has the disadvantage of biasing the accuracy metric towards true negatives, since there is a higher probability that a bird will not be present in most of the recordings. Since the ROC curve is plotting the true positive rate versus the false positive rate, each point on the ROC plot effectively represents *two* instances, one each from the correct and incorrect classification groups. Thus, if every example were marked as absent, the AUC would be 0.5. 

## II. Analysis

In this section, I present an analysis of the specific problem at hand, including a description of the dataset, the approach followed to solve the problem, an explanation of how the audio recognition problem is transformed into an image recognition problem, and finally, the specific machine learning techniques I tried to use for this problem, along with reasoning for them.

### Data Exploration
The training set consists of 323 wave files, representing sounds from 19 species of birds (see Table 1). Each recording contains up to 7 species of birds making sounds in them, maybe simultaneously.

There are 19 species of birds in the data set, indexed from 0 to 19, as tabulated in Table 1.

| Code | Name                       | 
|------|----------------------------| 
| 0    | Brown Creeper              | 
| 1    | Pacific Wren               | 
| 2    | Pacific-slope Flycatcher   | 
| 3    | Red-breasted Nuthatch      | 
| 4    | Dark-eyed Junco            | 
| 5    | Olive-sided Flycatcher     | 
| 6    | Hermit Thrush              | 
| 7    | Chestnut-backed Chickadee  | 
| 8    | Varied Thrush              | 
| 9    | Hermit Warbler             | 
| 10   | Swainson’s Thrush          | 
| 11   | Hammond’s Flycatcher       | 
| 12   | Western Tanager            | 
| 13   | Black-headed Grosbeak      | 
| 14   | Golden Crowned Kinglet     | 
| 15   | Warbling Vireo             | 
| 16   | MacGillivray’s Warbler     | 
| 17   | Stellar’s Jay              | 
| 18   | Common Nighthawk           | 
[Table 1: Bird species]

The dataset is quite unbalanced with respect to each of the species, as shown in Figure 1. In the region where the recordings were made, it looks like Swainson's Thrush was pretty common (more than 60 examples), while manyother birds are represented by less than 10 recordings. This means we need an algorithm capable of handling this kind of unbalanced dataset for a multiclass problem. 

![Figure 1: Number of examples in training set for each bird species](species_count_histogram.png)

Each data sample is a wave file, and hence that is not included in the report. Each wave file may or may not contain a bird sound, but they all contain noise from streams and wind and other sources, which means significant effort has to be expended to deal with this.

### Exploratory Visualization

As mentioned briefly in the preceding section, each recording can contain more than one recording, or even none. The presence of multiple birds in the same recording presents great challenges in terms of segmentation and source separation (the so-called cocktail party problem). A histogram of the number of species present in each recording is shown in Figure 2.

Most of the recordings have only none or one species present in them. However, approximately 30% of the recordings have more than one bird sound in them. The presence of more than one bird-sound with the possibility for overlap indicates that we will have to do some processing to the recordings to be able to separate the bird-sounds from each other.

![Figure 2: Histogram of number of birds in each recording](n_species_stats.png) 

### Algorithms and Techniques

I will first briefly describe the purpose of a spectrogram for audio, after which I will talk briefly about the processing techniques used in this algorithm. Both of them rely on first extracting the spectrogram of the audio from the audio file. 

A spectrogram is a visual representation of the frequencies present in an audio file. Most recognition problems (and work in many other areas) depend on a spectrogram since the frequencies present in an audio are, in fact, the things that define it. Also, using the frequencies allows us to automatically account for the effect of loudness since the frequency does not depend on loudness.

For sound recognition, there are two approaches that are popular:

1. Get a vector descriptive of the frequency range in the spectrogram and use that for recognition. One way to do this is convert the spectrogram to a Mel-Spectrum, i.e. apply a bunch of filters spaced logarithmically over the range of human frequencies to get a set of vectors for each of the syllables and/or sounds present in the audio. The vector can then be used as an input to a Machine Learning algorithm.
2. Use the spectrogram as an input to an image recognition problem, thus enabling the use of techniques such as segmentation and histogram-of-gradients. We can separate out sounds using segmentations and then each sound can be fed into a machine learning algorithm for classification.

For bird sound recognition, researchers have found that the second approach is better and so, that is the approach followed in this paper. Once the spectrogram is generated, a segmentation algorithm is run on the audio files to separate the individual bird sounds from the audio. The segmentation algorithm is trained using 20 of the wave files that have been manually segmented by humans. This data has been provided as part of the competition dataset. An example of the spectrogram (Figure 3) and the result after segmentation on the spectrogram (Figure 4) are provided below. The red segments indicate noise and the blue segments indicate a probable bird sound.

![Figure 3: Example Spectrogram](file:///Users/pritamps/Dropbox/birdsong/mlsp_contest_dataset/spectrograms/PC1_20090606_070012_0040.bmp)

![Figure 4: Segmented spectrogram](file:///Users/pritamps/Dropbox/birdsong/mlsp_contest_dataset/segmentation_examples/PC1_20090606_070012_0040.bmp)

After the segments are separated, a histogram-of-gradients feature is used to generate a feature vector for each of the segments. Note that the number of segments in an image may vary, so applying a machine learning algorithm to the segment features directly will be cumbersome. Instead of doing that, we generate bags-of-features from the histograms. Bags-of-features (commonly called bag-of-words) is a technique commonly used in text processing that provides information with how often a particular feature occurs in an instance. 

Basically, they are a way to accumulate information from different features into a single training instance. The output is a single fixed-length feature vector that can then be used for classification in a MIML problem. The final histogram-of-gradients features are provided along with the competition dataset and are 100x1 vector, and are what I used as input to my learning algorithm.

### Benchmark
The benchmark for this project was the score provided by the organisers of the competition: **0.85576**. The goal of this project will be to meet or exceed this benchmark. Another benchmark is that if we predict that there are no birds in any of the audio files, or predict the presence or absence perfectly at random, the area under the ROC curve will be **0.5**.

## III. Methodology

### Data Preprocessing
No preprocessing was needed as the complex transformations from the raw audio files to the filtered spectrograms was provided by the organisers of the competition. The feature vectors were extracted from the raw text files using the following code, which also further splits the data into testing and training sets:

import numpy as np
import pandas as pd

from sklearn.preprocessing import MultiLabelBinarizer


	def extract_data():
	    with open("../mlsp_contest_dataset/essential_data/rec_labels_test_hidden.txt") as f:
	        lines = f.readlines()
	
	    labels_train = []
	    training_ids = []
	    test_ids = []
	    for line in lines[1:]:
	        if "?" not in line:
	            x = map(int, line.split(','))
	            labels_train.append(x[1:])
	            training_ids.append(x[0])
	        else:
	            x = int(line.split(',')[0])
	            test_ids.append(x)
	
	    # Transform training_labels to nice format
	    labels_train = MultiLabelBinarizer().fit_transform(labels_train)
	    # Get the featuress
	    features_all = np.array(pd.read_csv('../mlsp_contest_dataset/supplemental_data/histogram_of_segments.txt', skiprows=1, index_col=0, header=None).values)
	    features_train = features_all[training_ids]
	    features_test = features_all[test_ids]
	
	    # Make sure sizes match
	    print "Data size: " + str(len(lines[1:]))
	    print "Feature set size: " + str(features_all.shape)
	    print "Test size: " + str(len(test_ids))
	    print "Training size: " + str(len(training_ids))
	    print "Training feature set size: " + str(features_train.shape)
	    print "Test feature set size: " + str(features_test.shape)
	    return features_train, features_test, labels_train, test_ids
    
### Implementation
Using the bag-of-words features provided with the dataset, I implemented a variety of machine-learning algorithms. The bag-of-words features clearly points to a Decision Tree type approach, but I wanted to compare the performance of different machine learning algorithms. I tried the following:

1. Random Forest Regressor
2. K-nearest neighbours
3. Logistic Regression
4. Ridge CV
5. Decision Trees

All of the above were implemented directly using the corresponding `sklearn` packages. 

### Refinement
Among the algorithms, the Random Forest Regressor performed the best by far. The parameters available for tuning in RFGs include the number of estimators, the maximum number of features and the random state variable. I tried a variety of combinations of these variables before approaching the benchmark score and exceeding it. The final parameters are briefly discussed in the Results section.


## IV. Results

### Model Evaluation and Validation


As mentioned in the Methodology section, I tried a variety of Machine Learning models to approach the problem, and found that using Random Forests exceeded the benchmark. A screenshot of the Kaggle results is shown in Figure 5.

The split of training-vs-testing (50/50) and the high score on the competition leaderboard shows us that the code does generalize well to unseen data (assuming it is in a similar format). Note that the dataset already has the following issues in it, that have bean addressed while preprocessing:

1. Noise
2. Absence of any bird sounds
3. Multiple bird sounds
4. Overlapping bird sounds

### Justification

![Figure 5: Results from the competition](kaggle_results.png)
The benchmark score was **0.85576**, and my score was **0.86081**, i.e. a small improvement over the original algorithm. It is, of course, significantly better than predicting the presence of birds at random.

## V. Conclusion

### Free-Form Visualization
This particular dataset does not lend itself to free-form visualisation, and I have already provided multiple visualisations of the data and the features in previous sections.

### Reflection
The problem that I attempted to solve can be summarised as follows: 

*Given a recording (pre-shortened to 10 s), predict if there are any birds in the recording, and if so, how many and what species (of a subset of 20 species*.

There are many challenges in going from audio to labeling of bird species, and through this project, I intended to understand these challenges and gain experience using the tools needed to overcome them as I move to a real-world setting. These challenges include:

1. Noise reduction
2. Separating overlapping bird sounds (the cocktail-party problem)
3. Localising birds sounds in the audio (segmentation)
4. Solving a Multi-Instance Multi-Label (MIML) problem

The final model as presented produced very impressive results. I am continuing to work on this topic and I hope to learn much more after finishing the MLND!

### Improvement

One significant improvement I did not get around to implementing is including the location of the recording as an input. It is especially valid for this problem as the microphones are placed in fixed locations in the forests of Oregon. However, even in general, adding location as a variable is probably a very good idea since bird species are highly localised.

Another improvement that I do not yet have enough technical background for, is using Ensemble Classifier Chains (ECCs) instead of Binary Relevance for the MIML classifications. ECCs are known to perform much better than Binary Relevance for MIML problems, and in the future, I would love to learn more about ECCs and use them for classification.